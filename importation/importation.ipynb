{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connexion à la base de données brutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect = sqlite3.connect(\"../Databases/raw-database.db\")\n",
    "cursor = connect.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Données de matching des matchs Skill Corner / Stats Bomb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture de l'unique fichier pour ces données\n",
    "matching_matches = pd.read_json(\"Projet_centres_data/matching_matches.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ecriture des données importées dans une table de la BDD\n",
    "matching_matches.to_sql(\"matching_matches\", con = connect, if_exists = \"replace\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Données de matching des joueurs Skill Corner / Stats Bomb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture de l'unique fichier pour ces données\n",
    "matching_players = pd.read_json(\"Projet_centres_data/matching_players.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "639"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ecriture des données importées dans une table de la BDD\n",
    "matching_players.to_sql(\"matching_players\", con = connect, if_exists = \"replace\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Données de matching des équipes Skill Corner / Stats Bomb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture de l'unique fichier pour ces données\n",
    "matching_teams = pd.read_json(\"Projet_centres_data/matching_teams.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ecriture des données importées dans une table de la BDD\n",
    "matching_teams.to_sql(\"matching_teams\", con = connect, if_exists = \"replace\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Informations sur les matchs Stats Bomb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture de l'unique fichier pour ces données\n",
    "SB_matches = pd.read_json(\"Projet_centres_data/SB_matches.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ecriture des données importées dans une table de la BDD\n",
    "SB_matches.to_sql(\"SB_matches\", con = connect, if_exists = \"replace\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Données events de Stats Bomb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ces données, nous avons un fichier par match, il faut donc importer une liste de fichiers (situés dans le dossier SB_events).  \n",
    "Pour ce faire, nous allons ouvrir un par un ces fichiers et les concaténer afin d'obtenir un unique jeu de données comprenant l'ensemble des fichiers.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des noms des fichiers à ouvrir\n",
    "liste_fichier_events = os.listdir(\"Projet_centres_data/SB_events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commande qui permet de concaténer l'ensemble des fichiers\n",
    "# La boucle for à l'intérieur des [] permet de parcourir la liste des noms des fichiers, et d'ouvrir un fichier un chaque itération\n",
    "events = pd.concat([pd.read_json(\"Projet_centres_data/SB_events/\" + fichier_event) for fichier_event in liste_fichier_events])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant filtrer ces données afin de :  \n",
    "- Garder uniquement les données dont nous nous servirons. En effet, ces données contiennent 108 variables(colonnes) de base, or elles ne nous sont pas toutes utiles, nous allons donc garder seulement celles qui nous sont utiles afin de réduire la taille des données.  \n",
    "- Renommer certaines colonnes afin de faciliter notre utilisation de ces dernières.  \n",
    "- Convertir les colonnes contenant des données stockées sous forme de liste en plusieurs colonnes comprenant chacune un élément de la liste.  \n",
    "En effet, les bases de données SQLite ne supportent pas les données de type liste, dictionnaire, dataframe etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste contenant les noms de colonnes que nous souhaitons garder\n",
    "colonnes = [\"id\", \"shot_type\", \"shot_outcome\", \"type\", \"match_id\", \"period\", \"possession\", \"location\", \"pass_cross\", \"pass_type\", \"index\",\n",
    "            \"pass_end_location\", \"minute\", \"shot_end_location\", \"pass_body_part\", \"player_id\"]\n",
    "\n",
    "# Dictionnaire permettant de renommer les colonnes souhaitées\n",
    "dico_rename = {\"index\" : \"index_event\", \"id\" : \"event_id\", \"match_id\" : \"match_id_SB\"}\n",
    "\n",
    "# Commande pour garder uniquement colonnes souhaitées et renommer les colonnes souhaitées\n",
    "# De plus, nous utilisons la commande reset_index afin d'obtenir un unique identifiant pour chaque evenement de l'ensemble des données concaténées.\n",
    "# Nous disposons déjà de la variable event_id qui est unique à chaque event, sauf qu'elle n'est pas \"intuitive\" et \"lisible\" car c'est une chaine\n",
    "# de caractère créée \"aléatoirement\".\n",
    "# Ce nouvel index est une range = (0, nombre d'events), et ces events sont triés par ordre chronologique par match dans les données\n",
    "# concaténées, ce qui est plus intuitif.\n",
    "# L'event d'index 0 correspondra donc au premier évènement survenu lors du premier match\n",
    "events = events[colonnes].reset_index(drop = True).rename(dico_rename, axis = 1)\n",
    "\n",
    "# Les données de la colonne \"location\" sont des listes de 3 coordonnées, nous allons donc \"éclater\" cette colonne en 3 colonnes :\n",
    "# \"x_loc\", \"y_loc\" et \"z_loc\".\n",
    "# Pour ce faire, nous extrayons d'abord la colonne du dataframe, puis nous créons un autre dataframe comprenant les 3 colonnes des coordonnées\n",
    "events_loc = events.pop(\"location\").dropna()\n",
    "events_loc = pd.DataFrame(events_loc.tolist(), index = events_loc.index, columns = [\"x_loc\", \"y_loc\", \"z_loc\"])\n",
    "\n",
    "# De même pour la colonne \"pass_end_location\", sauf que celle-ci ne contient que 2 coordonnées\n",
    "events_pass_loc = events.pop(\"pass_end_location\").dropna()\n",
    "events_pass_loc = pd.DataFrame(events_pass_loc.tolist(), index = events_pass_loc.index, columns = [\"x_pass\", \"y_pass\"])\n",
    "\n",
    "# De même pour la colonne \"shot_end_location\" qui comprend 3 coordonnées\n",
    "events_shot_loc = events.pop(\"shot_end_location\").dropna()\n",
    "events_shot_loc = pd.DataFrame(events_shot_loc.tolist(), index = events_shot_loc.index, columns = [\"x_shot\", \"y_shot\", \"z_shot\"])\n",
    "\n",
    "# Il nous reste a concaténer les 3 dataframes créés précédemment avec le dataframe initial.\n",
    "# Nous concaténons ces dataframes horizontalement (nous les ajoutons à droite du dataframe initial)\n",
    "events = pd.concat([events, events_loc, events_pass_loc, events_shot_loc], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1118352"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ecriture des données importées dans une table de la BDD\n",
    "events.to_sql(\"events\", con = connect, if_exists = \"replace\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Données freeze frame de Skill Corner sur les centres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De même que pour les données event, nous disposons de 1 fichier par match pour ces données, nous allons donc concaténer l'ensemble des données de chaque fichier dans un seul jeu de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des noms des fichiers à ouvrir\n",
    "liste_fichier_freeze_frames = os.listdir(\"Projet_centres_data/SKC_crosses_freeze_frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cependant, cette fois nous souhaitons aussi conserver l'information \"match_id\" (Skill Corner) pour chaque match, qui n'est pas initialement pas présente dans les données \"freeze frames\".  \n",
    "Cette information est en fait stockée dans le nom des fichiers qui sont de la forme \"match_id.json\".  \n",
    "Pour ce faire, nous allons, grâce à une boucle for : \n",
    "- Parcourir la liste des noms des fichiers à lire (le nom correspond au \"match_id\" Skill Corner)\n",
    "- Lire le fichier correspondant\n",
    "- Ajouter l'information \"match_id_SKC\" dans une nouvelle colonne (le \"match_id\" sera ajouté à toutes les lignes du dataframe)\n",
    "- Concaténer le jeu de données (dataframe) obtenu lors de l'itération au jeu de données comprenant l'ensemble des données des fichier lus lors des précédentes itérations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dataframe qui contiendra le jeu de données concaténer (les données de l'ensemble des fichiers de données \"freeze frames\")\n",
    "freeze_frames = pd.DataFrame()\n",
    "\n",
    "# Boucle itérative\n",
    "for fichier_freeze_frames in liste_fichier_freeze_frames :\n",
    "    # Lecture du fichier correspondant\n",
    "    freeze_frames_import = pd.read_json(\"Projet_centres_data/SKC_crosses_freeze_frames/\" + fichier_freeze_frames)\n",
    "    \n",
    "    # Création de la colonne \"match_id_SKC\"\n",
    "    # Nous enlevons la partie \".json\" de la chaine de caractères correspondant au nom du fichier\n",
    "    freeze_frames_import[\"match_id_SKC\"] = int(fichier_freeze_frames.replace(\".json\", \"\"))\n",
    "\n",
    "    # Concaténation du dataframe obtenu au dataframe comprenenant le jeu de données\n",
    "    freeze_frames = pd.concat([freeze_frames, freeze_frames_import])\n",
    "\n",
    "# Pour les mêmes raisons que pour les données events, nous allons créer un nouvel index pour identifier les freeze frames.\n",
    "freeze_frames.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Nous supprimons la variable \"image_corners_projection\" qui nous est inutile\n",
    "freeze_frames.drop(\"image_corners_projection\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut traiter la colonne \"possession\", qui contient des dictionnaires comprenant chacuns le joueur et l'équipe en possession du ballon au moment ou la frame a été capturée.  \n",
    "En effet, ces dictionnaires ne peuvent pas être stocké par SQLite...  \n",
    "Pour ce faire, nous allons procéder comme pour les listes de coordonnées précédentes :\n",
    "- Supprimer et récupérer la colonne du dataframe\n",
    "- Eclater cette colonne en 2 nouvelles colonnes qui contiennent respectivement l'équipe en possession du ballon et le joueur en possession du ballon lors de la frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_frames_possession = freeze_frames.pop(\"possession\")\n",
    "freeze_frames[[\"group\", \"tackable_object\"]] = pd.json_normalize(freeze_frames_possession)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant traiter la colonne \"data\", qui contient pour chaque frame des informations (position, vitesse, etc) pour chaque joueur et le ballon.  \n",
    "En effet, pour chaque frame, la valeur de cette colonne est en fait une liste de dictionnaire.  \n",
    "Chaque dictionnaire de la liste correspond aux informations (position, vitesse, etc) pour un joueur ou le ballon.  \n",
    "De ce fait, les données de cette colonne \"data\" ne peuvent pas être stockées par SQLite.  \n",
    "Cependant, nous pouvons voir chaque élèment de cette colonne comme un dataframe, car ces élèments sont des listes de dictionnaires (possédant tous les même clés).  \n",
    "De ce fait, en convertissant chaque ligne de cette colonne, nous obtiendrons donc (nombre de frames) dataframes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pour effectuer cette manipulation, nous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_freeze_frames_data(row) :\n",
    "    df = pd.DataFrame(row.data)\n",
    "    df[\"frame\"] = row.frame\n",
    "    df[\"match_id_SKC\"] = row.match_id_SKC\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "freeze_frames_data = pd.concat((freeze_frames.apply(transform_freeze_frames_data, axis = 1)).tolist())\n",
    "freeze_frames.drop(\"data\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20576"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freeze_frames.to_sql(\"freeze_frames\", con = connect, if_exists = \"replace\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467914"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freeze_frames_data.to_sql(\"freeze_frames_data\", con = connect, if_exists = \"replace\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Données off ball runs de Skill Corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_fichier_off_ball_runs = os.listdir(\"Projet_centres_data/SKC_off_ball_runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "off_ball_runs = pd.concat([pd.read_json(\"Projet_centres_data/SKC_off_ball_runs/\" + fichier_off_ball_runs)\n",
    "                           for fichier_off_ball_runs in liste_fichier_off_ball_runs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8489"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "off_ball_runs.to_sql(\"off_ball_runs\", con = connect, if_exists = \"replace\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching pour chaque joueur entre son player_id et son team_id (au sens de Stats Bomb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_fichier_lineups = os.listdir(\"Projet_centres_data/SB_lineups\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineups = pd.concat([pd.read_json(\"Projet_centres_data/SB_lineups/\" + fichier_lineups)\n",
    "                           for fichier_lineups in liste_fichier_lineups])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fermeture de la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "connect.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_PROJET",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
